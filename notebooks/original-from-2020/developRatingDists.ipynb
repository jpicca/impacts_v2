{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import skimage.morphology as skmorph\n",
    "import os\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "from pygridder import Gridder\n",
    "import pygrib as pg\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dx = 5 # delta x\n",
    "selem = skmorph.disk(40 / dx) # morphology disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfd_path = pathlib.Path('..','..','impacts-data','pas-input-data','ndfd.npz').resolve()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with np.load(ndfd_path) as NPZ:\n",
    "    lons = NPZ['lons']\n",
    "    lats = NPZ['lats']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Gridder(tx=lons, ty=lats, dx=dx/100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read in onetor data and prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = pathlib.Path('..','raw-tor-data','1950-2018_actual_tornadoes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dateparser = lambda x: dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') + dt.timedelta(hours=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, parse_dates=[['date','time']], date_parser=dateparser, index_col=0, keep_date_col=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.sg == 1]\n",
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "di = {0: 1.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0}\n",
    "df['weight'] = df['mag'].map(di).fillna(df['mag'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlook_time = '1200'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create date array\n",
    "my_hour = dt.datetime.strptime(outlook_time, '%H%M').hour\n",
    "my_minute = dt.datetime.strptime(outlook_time, '%H%M').minute\n",
    "\n",
    "bdt = dt.datetime(1999,5,3,my_hour,my_minute)\n",
    "edt = dt.datetime(1999,5,4,my_hour,my_minute)\n",
    "\n",
    "# Create list index of datetimes with a frequency of one per day\n",
    "dts = pd.date_range(bdt, edt, freq='D')\n",
    "bdts, edts = dts[:-1],dts[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check outlook/PP data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = [\n",
    "    {\n",
    "        'type': 'pp',\n",
    "        'start': '11041965',\n",
    "        'end': '12041965'\n",
    "    },\n",
    "    {\n",
    "        'type': 'pp',\n",
    "        'start': '03041974',\n",
    "        'end': '04041974'\n",
    "    },\n",
    "    {\n",
    "        'type': 'pp',\n",
    "        'start': '31051985',\n",
    "        'end': '01061985'\n",
    "    },\n",
    "    {\n",
    "        'type': 'pp',\n",
    "        'start': '03051999',\n",
    "        'end': '04051999'\n",
    "    },\n",
    "    {\n",
    "        'type': 'hum',\n",
    "        'start': '27042011',\n",
    "        'end': '28042011'\n",
    "    }    \n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "G = Gridder(tx=lons, ty=lats, dx=dx/100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists = []\n",
    "\n",
    "for date in dates:\n",
    "    # Start times\n",
    "    sday = int(date['start'][:2].lstrip(\"0\"))\n",
    "    smon = int(date['start'][2:4].lstrip(\"0\"))\n",
    "    syear = int(date['start'][4:])\n",
    "    \n",
    "    # End times\n",
    "    eday = int(date['end'][:2].lstrip(\"0\"))\n",
    "    emon = int(date['end'][2:4].lstrip(\"0\"))\n",
    "    eyear = int(date['end'][4:])\n",
    "    \n",
    "    # Create date array\n",
    "    my_hour = dt.datetime.strptime(outlook_time, '%H%M').hour\n",
    "    my_minute = dt.datetime.strptime(outlook_time, '%H%M').minute\n",
    "\n",
    "    bdt = dt.datetime(syear,smon,sday,my_hour,my_minute)\n",
    "    edt = dt.datetime(eyear,emon,eday,my_hour,my_minute)\n",
    "\n",
    "    # Create list index of datetimes with a frequency of one per day\n",
    "    dts = pd.date_range(bdt, edt, freq='D')\n",
    "    bdts, edts = dts[:-1],dts[1:]\n",
    "    \n",
    "    edts = edts.map(lambda x: x.replace(hour=12,minute=0))\n",
    "    \n",
    "    if date['type'] == 'pp':\n",
    "        regfile = f'../pp_forecasts/regProbs/{syear}{date[\"start\"][2:4]}{date[\"start\"][:2]}_1630.npz'\n",
    "        reg_probs = np.load(regfile)['fcst']\n",
    "        \n",
    "        sigfile = f'../pp_forecasts/sigProbs/{syear}{date[\"start\"][2:4]}{date[\"start\"][:2]}_1630.npz'\n",
    "        sig_probs = np.load(sigfile)['fcst']\n",
    "    else:\n",
    "        regfile = '../../sampleinputs/torn_day1_grib2_1200_20110427061047'\n",
    "        with pg.open(regfile) as GRB:\n",
    "            try:\n",
    "                reg_probs = GRB[1].values.filled(-1)\n",
    "            except AttributeError:\n",
    "                reg_probs = GRB[1].values\n",
    "        sigfile = '../../sampleinputs/sigtorn_day1_grib2_1200_20110427061047'\n",
    "        with pg.open(sigfile) as GRB:\n",
    "            try:\n",
    "                sig_probs = GRB[1].values.filled(-1)\n",
    "            except AttributeError:\n",
    "                sig_probs = GRB[1].values\n",
    "                \n",
    "        reg_probs = reg_probs/100\n",
    "        sig_probs = sig_probs/100\n",
    "    \n",
    "    ##\n",
    "    ## Collect tornadoes in sig/mod\n",
    "    ##\n",
    "    \n",
    "    double_sig_tors = []\n",
    "    \n",
    "    _df = df[(df['date_time'] >= bdts[0]) & (df['date_time'] < edts[0]) & (df['weight'] == 1.0)]\n",
    "    if _df.empty:\n",
    "        continue\n",
    "    # print(f'From {bdt:%Y-%m-%d %H%M}z to {edt:%Y-%m-%d %H%M}z')\n",
    "    \n",
    "    lon1 = _df.slon.values\n",
    "    lat1 = _df.slat.values\n",
    "    lon2 = _df.elon.values\n",
    "    lat2 = _df.elat.values\n",
    "    \n",
    "    # Find/remove/replace missing data\n",
    "    keep = ~np.logical_or(lon1 == 0, lat1 == 0)\n",
    "    lon1 = lon1[keep]\n",
    "    lat1 = lat1[keep]\n",
    "    lon2 = lon2[keep]\n",
    "    lat2 = lat2[keep]\n",
    "    lon2[lon2 == 0] = lon1[lon2 == 0]\n",
    "    lat2[lat2 == 0] = lat1[lat2 == 0]\n",
    "    \n",
    "    # Mask sig grid where tor probs are below MDT threshold (less than 0.15)\n",
    "    sig_probs = np.ma.masked_where(reg_probs < 0.15, sig_probs)\n",
    "    \n",
    "    # Loop through tornadoes\n",
    "    for i in range(len(lat1)):\n",
    "        # Grid tornadoes\n",
    "        tornlines = G.grid_lines(sxs=lon1[i], sys=lat1[i], exs=lon2[i], eys=lat2[i])\n",
    "        mags = _df['weight']\n",
    "        fcst = G.make_empty_grid(dtype='float')\n",
    "        for tornline, mag in zip(tornlines, mags):\n",
    "            fcst[tornline] = mag\n",
    "        \n",
    "        if (np.max(np.ma.masked_where(sig_probs<0.05,fcst)) > 0):\n",
    "        #if (np.max(np.ma.masked_where(vals<0.05,fcst)) > 0):\n",
    "            double_sig_tors.append(i)\n",
    "            \n",
    "    rating_dist = _df.iloc[double_sig_tors,:].groupby('mag').count()['weight']/len(double_sig_tors)\n",
    "    \n",
    "    dists.append(rating_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Distribution: mag\n",
      "0    0.187347\n",
      "1    0.250486\n",
      "2    0.170708\n",
      "3    0.167871\n",
      "4    0.193852\n",
      "5    0.029736\n",
      "Name: weight, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "tot = dists[0]\n",
    "\n",
    "for dist in dists[1:]:\n",
    "   tot = tot.add(dist,fill_value=0)\n",
    "\n",
    "print(f'Final Distribution: {tot/5}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
