{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating model climo of impacts\n",
    "### National, State, CWA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Create index of days through the year [0...366]\n",
    "- For each day, we have {Natl: {min:, 10:, 50:, 90:, max:}, {States: {AL: {min:, 10:, 50:, 90:, max:}}, {cwas: {ALY: {min:, 10:, 50:, 90:, max:}}}\n",
    "- Open each sim file, grab day and calculate stats for nat, states, and cwas\n",
    "- Add the quantiles to the running total for each\n",
    "- Divide by the number of total years for each day to acquire the average of each stat \n",
    "- Run a 7-day running average on the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "import glob\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob('/Volumes/Backup Plus/pas_output/simulationFiles/*')\n",
    "\n",
    "# Takes the file suffix and grabs the date index\n",
    "#datetime.datetime.strptime(files[-180].split('/')[-1].split('_')[0][:-4],'%Y%m%d').strftime('%j')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grab state and cwa lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../visualization/exampleDashboard/includes/jsons/states.json') as json_file: \n",
    "    states = json.load(json_file)\n",
    "\n",
    "with open('../visualization/exampleDashboard/includes/jsons/cwa.json') as json_file:\n",
    "    cwas = json.load(json_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up large dictionary to hold quantile sums"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeStatHolder():\n",
    "    \n",
    "    with open('../visualization/exampleDashboard/includes/jsons/states.json') as json_file: \n",
    "        states = json.load(json_file)\n",
    "\n",
    "    with open('../visualization/exampleDashboard/includes/jsons/cwa.json') as json_file:\n",
    "        cwas = json.load(json_file)\n",
    "\n",
    "    statHolder = {'nat': {'min':np.zeros(366),'ten':np.zeros(366),'med':np.zeros(366),'ninety':np.zeros(366),'max':np.zeros(366)},'states': {}, 'cwas': {}}\n",
    "\n",
    "    for state in states:\n",
    "        statHolder['states'][state['abbreviation']] = {'min':np.zeros(366),'ten':np.zeros(366),'med':np.zeros(366),'ninety':np.zeros(366),'max':np.zeros(366)}\n",
    "\n",
    "    for cwa in cwas:\n",
    "        statHolder['cwas'][cwa['abbreviation']] = {'min':np.zeros(366),'ten':np.zeros(366),'med':np.zeros(366),'ninety':np.zeros(366),'max':np.zeros(366)}\n",
    "        \n",
    "        #print(cwa['abbreviation'])\n",
    "    return statHolder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the main loop to run our climo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 file processing\n",
      "100 file processing\n",
      "200 file processing\n",
      "300 file processing\n",
      "400 file processing\n",
      "500 file processing\n",
      "600 file processing\n",
      "700 file processing\n",
      "800 file processing\n",
      "900 file processing\n",
      "1000 file processing\n",
      "1100 file processing\n",
      "1200 file processing\n",
      "1300 file processing\n",
      "1400 file processing\n",
      "1500 file processing\n",
      "1600 file processing\n",
      "1700 file processing\n",
      "1800 file processing\n",
      "1900 file processing\n",
      "2000 file processing\n",
      "2100 file processing\n",
      "2200 file processing\n",
      "2300 file processing\n",
      "2400 file processing\n",
      "2500 file processing\n",
      "2600 file processing\n",
      "2700 file processing\n",
      "2800 file processing\n",
      "2900 file processing\n",
      "3000 file processing\n",
      "3100 file processing\n",
      "3200 file processing\n",
      "3300 file processing\n",
      "3400 file processing\n",
      "3500 file processing\n",
      "3600 file processing\n",
      "3700 file processing\n",
      "3800 file processing\n",
      "3900 file processing\n",
      "4000 file processing\n",
      "4100 file processing\n",
      "4200 file processing\n",
      "4300 file processing\n",
      "4400 file processing\n",
      "4500 file processing\n",
      "4600 file processing\n",
      "4700 file processing\n",
      "4800 file processing\n",
      "4900 file processing\n",
      "5000 file processing\n",
      "5100 file processing\n",
      "5200 file processing\n",
      "5300 file processing\n",
      "5400 file processing\n",
      "5500 file processing\n",
      "5600 file processing\n",
      "5700 file processing\n",
      "5800 file processing\n",
      "5900 file processing\n",
      "6000 file processing\n",
      "6100 file processing\n",
      "6200 file processing\n",
      "6300 file processing\n",
      "6400 file processing\n",
      "6500 file processing\n",
      "6600 file processing\n",
      "6700 file processing\n",
      "6800 file processing\n",
      "6900 file processing\n",
      "7000 file processing\n",
      "7100 file processing\n",
      "7200 file processing\n",
      "7300 file processing\n",
      "7400 file processing\n",
      "7500 file processing\n",
      "7600 file processing\n",
      "7700 file processing\n",
      "7800 file processing\n",
      "7900 file processing\n",
      "8000 file processing\n",
      "8100 file processing\n",
      "8200 file processing\n",
      "8300 file processing\n",
      "8400 file processing\n",
      "8500 file processing\n",
      "8600 file processing\n",
      "8700 file processing\n",
      "8800 file processing\n",
      "8900 file processing\n",
      "9000 file processing\n",
      "9100 file processing\n",
      "9200 file processing\n",
      "9300 file processing\n",
      "9400 file processing\n",
      "9500 file processing\n",
      "9600 file processing\n",
      "9700 file processing\n",
      "9800 file processing\n",
      "9900 file processing\n",
      "10000 file processing\n",
      "10100 file processing\n",
      "10200 file processing\n",
      "10300 file processing\n",
      "10400 file processing\n",
      "10500 file processing\n",
      "10600 file processing\n",
      "10700 file processing\n",
      "10800 file processing\n",
      "10900 file processing\n",
      "11000 file processing\n",
      "11100 file processing\n"
     ]
    }
   ],
   "source": [
    "#statHolder = {'nat': {'min':np.zeros(366),'ten':np.zeros(366),'med':np.zeros(366),'ninety':np.zeros(366),'max':np.zeros(366)},'states': {}, 'cwas': {}}\n",
    "\n",
    "statHolder = makeStatHolder()\n",
    "\n",
    "impact = 'psubstations'\n",
    "\n",
    "for i,file in enumerate(files):\n",
    "    \n",
    "    if (i%100 == 0):\n",
    "        print(f'{i} file processing')\n",
    "    \n",
    "    # Get the index of the day and then subtract 1 for 0-indexing\n",
    "    date = datetime.datetime.strptime(file.split('/')[-1].split('_')[0][:-4],'%Y%m%d')\n",
    "    dayIdx = int(date.strftime('%j'))-1\n",
    "    # If not a leap year and the dayIdx is beyond what would be the leap day, add 1 to the dayIdx\n",
    "    if ((date.year%4 != 0) and (dayIdx > 58)):\n",
    "        dayIdx += 1\n",
    "    \n",
    "    # Set up national stats\n",
    "    df = pd.read_csv(file, sep=\"|\")\n",
    "    sims = df.groupby(\"sim\")\n",
    "    fields = sims.sum().loc[:,('population','hospitals','mobilehomes','psubstations')]\n",
    "    # Fill missing sims with 0s\n",
    "    fill_fields = fields.reindex(list(range(1,10001)),fill_value=0)\n",
    "    \n",
    "    # Get national quantiles for this file\n",
    "    quants = fields.describe(percentiles=[0.1,0.5,0.9]).loc[('min','10%','50%','90%','max'),[impact]][impact].values.tolist()\n",
    "    \n",
    "    # Add quantiles to the proper slot in the national sub-dictionary\n",
    "    statHolder['nat']['min'][int(dayIdx)] += quants[0]\n",
    "    statHolder['nat']['ten'][int(dayIdx)] += quants[1]\n",
    "    statHolder['nat']['med'][int(dayIdx)] += quants[2]\n",
    "    statHolder['nat']['ninety'][int(dayIdx)] += quants[3]\n",
    "    statHolder['nat']['max'][int(dayIdx)] += quants[4]\n",
    "    \n",
    "    \n",
    "    ##### States #####\n",
    "    \n",
    "    stBrokenOut = df.assign(category=df['states'].str.split(',')).explode('category').reset_index(drop=True)\n",
    "\n",
    "    # Remove row if state is NaN\n",
    "    stBrokenOut = stBrokenOut[stBrokenOut['category'].notna()]\n",
    "\n",
    "    # Grab a list of the unique states in the simulation\n",
    "    statesImpacted = stBrokenOut['category'].unique().tolist()\n",
    "    \n",
    "    # Loop through impacted states\n",
    "    for state in statesImpacted:\n",
    "\n",
    "        # Organize dataframe for state and fill no-impact sims with 0s\n",
    "        ind_state = stBrokenOut[stBrokenOut['category'] == state]\n",
    "        #print(f'State: {state}')\n",
    "        ind_state_group = ind_state.groupby(\"sim\").sum().loc[:,['population','hospitals','mobilehomes','psubstations']]\n",
    "        ind_state_tot = ind_state_group.reindex(list(range(1,10001)),fill_value=0)\n",
    "        \n",
    "        # Get state quantiles for this file\n",
    "        quants = ind_state_tot.describe(percentiles=[0.1,0.5,0.9]).loc[['min','10%','50%','90%','max'],[impact]][impact].values.tolist()\n",
    "        \n",
    "        # Add quantiles to the proper slot in the national sub-dictionary\n",
    "        statHolder['states'][state]['min'][int(dayIdx)] += quants[0]\n",
    "        statHolder['states'][state]['ten'][int(dayIdx)] += quants[1]\n",
    "        statHolder['states'][state]['med'][int(dayIdx)] += quants[2]\n",
    "        statHolder['states'][state]['ninety'][int(dayIdx)] += quants[3]\n",
    "        statHolder['states'][state]['max'][int(dayIdx)] += quants[4]\n",
    "        \n",
    "    ##### CWAs #####\n",
    "    \n",
    "    cwaBrokenOut = df.assign(category=df['wfos'].str.split(',')).explode('category').reset_index(drop=True)\n",
    "\n",
    "    # Remove row if state is NaN\n",
    "    cwaBrokenOut = cwaBrokenOut[cwaBrokenOut['category'].notna()]\n",
    "\n",
    "    # Grab a list of the unique states in the simulation\n",
    "    cwasImpacted = cwaBrokenOut['category'].unique().tolist()\n",
    "    \n",
    "    # Loop through impacted states\n",
    "    for cwa in cwasImpacted:\n",
    "\n",
    "        # Organize dataframe for state and fill no-impact sims with 0s\n",
    "        ind_cwa = cwaBrokenOut[cwaBrokenOut['category'] == cwa]\n",
    "        #print(f'State: {state}')\n",
    "        ind_cwa_group = ind_cwa.groupby(\"sim\").sum().loc[:,['population','hospitals','mobilehomes','psubstations']]\n",
    "        ind_cwa_tot = ind_cwa_group.reindex(list(range(1,10001)),fill_value=0)\n",
    "        \n",
    "        # Get state quantiles for this file\n",
    "        quants = ind_cwa_tot.describe(percentiles=[0.1,0.5,0.9]).loc[['min','10%','50%','90%','max'],[impact]][impact].values.tolist()\n",
    "        \n",
    "        try:\n",
    "            # Add quantiles to the proper slot in the national sub-dictionary\n",
    "            statHolder['cwas'][cwa]['min'][int(dayIdx)] += quants[0]\n",
    "            statHolder['cwas'][cwa]['ten'][int(dayIdx)] += quants[1]\n",
    "            statHolder['cwas'][cwa]['med'][int(dayIdx)] += quants[2]\n",
    "            statHolder['cwas'][cwa]['ninety'][int(dayIdx)] += quants[3]\n",
    "            statHolder['cwas'][cwa]['max'][int(dayIdx)] += quants[4]\n",
    "        except:\n",
    "            print(file)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divide the np arrays by the number of years for each position\n",
    "Normal: 68\n",
    "Leap: 17\n",
    "Divide everything by 68, but then multiply the 59th index (leap day) by 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to return the moving average from the input array and the moving window size\n",
    "def movingaverage(array,window_size):\n",
    "    \n",
    "    subset_size = math.floor(window_size/2)\n",
    "    new_arr = np.insert(array,0,array[-subset_size:])\n",
    "    to_process = np.append(new_arr,array[:subset_size])\n",
    "    \n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(to_process,window,'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A dictionary to hold the raw averages\n",
    "averages = makeStatHolder()\n",
    "smoothed_averages = makeStatHolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries to hold averages\n",
    "averages = makeStatHolder()\n",
    "smoothed_averages = makeStatHolder()\n",
    "\n",
    "# Constants\n",
    "tot_years = 68\n",
    "leap_years = 17\n",
    "window_size = 31\n",
    "\n",
    "for i,key in enumerate(statHolder.keys()):\n",
    "    if i == 0:\n",
    "        quantKeys = statHolder[key].keys()\n",
    "        for innerKey in quantKeys:\n",
    "            # Create averages\n",
    "            averages[key][innerKey] = statHolder[key][innerKey]/tot_years\n",
    "            \n",
    "            # Correct leap year average by multiplying by 4\n",
    "            averages[key][innerKey][59] = averages[key][innerKey][59]*(tot_years/leap_years)\n",
    "            \n",
    "            # Create smoothed averages\n",
    "            smoothed_averages[key][innerKey] = movingaverage(averages[key][innerKey],window_size)\n",
    "            \n",
    "    elif i == 1:\n",
    "        states = statHolder[key].keys()\n",
    "        for state in states:\n",
    "            quantKeys = statHolder[key][state].keys()\n",
    "            for innerKey in quantKeys:\n",
    "                #statHolder[key][state][innerKey] = statHolder[key][state][innerKey].tolist()\n",
    "                # Create averages\n",
    "                averages[key][state][innerKey] = statHolder[key][state][innerKey]/tot_years\n",
    "\n",
    "                # Correct leap year average by multiplying by 4\n",
    "                averages[key][state][innerKey][59] = averages[key][state][innerKey][59]*(tot_years/leap_years)\n",
    "                \n",
    "                # Create smoothed averages\n",
    "                smoothed_averages[key][state][innerKey] = movingaverage(averages[key][state][innerKey],window_size)\n",
    "                \n",
    "    else:\n",
    "        cwas = statHolder[key].keys()\n",
    "        for cwa in cwas:\n",
    "            quantKeys = statHolder[key][cwa].keys()\n",
    "            for innerKey in quantKeys:\n",
    "                #statHolder[key][cwa][innerKey] = statHolder[key][cwa][innerKey].tolist()\n",
    "                \n",
    "                # Create averages\n",
    "                averages[key][cwa][innerKey] = statHolder[key][cwa][innerKey]/tot_years\n",
    "\n",
    "                # Correct leap year average by multiplying by 4\n",
    "                averages[key][cwa][innerKey][59] = averages[key][cwa][innerKey][59]*(tot_years/leap_years)\n",
    "                \n",
    "                # Create smoothed averages\n",
    "                smoothed_averages[key][cwa][innerKey] = movingaverage(averages[key][cwa][innerKey],window_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_arr = np.array([1,2,3,4,5,6,7,8,9,19])\n",
    "window_size = 7\n",
    "\n",
    "def movingaverage(array,window_size):\n",
    "    \n",
    "    subset_size = math.floor(window_size/2)\n",
    "    new_arr = np.insert(array,0,array[-subset_size:])\n",
    "    to_process = np.append(new_arr,array[:subset_size])\n",
    "    \n",
    "    window = np.ones(int(window_size))/float(window_size)\n",
    "    return np.convolve(to_process,window,'valid')\n",
    "    \n",
    "#np.convolve(test_arr,np.ones(int(window_size))/float(window_size),'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.2272296 , 0.23055028, 0.2272296 , 0.22912713, 0.23766603,\n",
       "       0.25711575, 0.27371917, 0.2528463 , 0.25616698, 0.24952562,\n",
       "       0.24762808, 0.2414611 , 0.23956357, 0.25237192, 0.25      ,\n",
       "       0.24573055, 0.24383302, 0.23197343, 0.22343454, 0.22248577,\n",
       "       0.24240987, 0.24098672, 0.23197343, 0.22865275, 0.24240987,\n",
       "       0.23529412, 0.23766603, 0.24051233, 0.23719165, 0.23719165,\n",
       "       0.24620493, 0.25332068, 0.26470588, 0.278463  , 0.27656546,\n",
       "       0.28605313, 0.2841556 , 0.27087287, 0.2841556 , 0.30597723,\n",
       "       0.30597723, 0.31688805, 0.3240038 , 0.35673624, 0.36859583,\n",
       "       0.39611006, 0.43121442, 0.44117647, 0.45683112, 0.47580645,\n",
       "       0.50237192, 0.49288425, 0.50996205, 0.51185958, 0.53368121,\n",
       "       0.5370019 , 0.5683112 , 0.59535104, 0.60009488, 0.61100569,\n",
       "       0.62855787, 0.62049336, 0.63614801, 0.63187856, 0.65607211,\n",
       "       0.67077799, 0.66461101, 0.65464896, 0.66413662, 0.66318786,\n",
       "       0.65654649, 0.6902277 , 0.7243833 , 0.7414611 , 0.73339658,\n",
       "       0.72960152, 0.73244782, 0.77324478, 0.87476281, 0.89611006,\n",
       "       0.91935484, 0.90796964, 0.93572106, 0.97841556, 1.00735294,\n",
       "       1.03439279, 1.07329222, 1.05194497, 1.04814991, 1.0766129 ,\n",
       "       1.09843454, 1.1363852 , 1.15203985, 1.16295066, 1.26114801,\n",
       "       1.25213472, 1.28059772, 1.30811195, 1.3379981 , 1.35744782,\n",
       "       1.38164137, 1.44995256, 1.45042694, 1.45896584, 1.48553131,\n",
       "       1.50925047, 1.53296964, 1.53629032, 1.49928843, 1.45706831,\n",
       "       1.48173624, 1.48932638, 1.54530361, 1.58491461, 1.57020873,\n",
       "       1.62381404, 1.61859583, 1.59440228, 1.62618596, 1.63757116,\n",
       "       1.66223909, 1.66508539, 1.64089184, 1.68406072, 1.69734345,\n",
       "       1.63567362, 1.62618596, 1.61005693, 1.61100569, 1.63709677,\n",
       "       1.67931689, 1.70018975, 1.66982922, 1.66888046, 1.65607211,\n",
       "       1.67172676, 1.6902277 , 1.66223909, 1.68643264, 1.6816888 ,\n",
       "       1.65702087, 1.63899431, 1.62286528, 1.60958254, 1.58776091,\n",
       "       1.56593928, 1.50711575, 1.51755218, 1.53083491, 1.53178368,\n",
       "       1.51802657, 1.48339658, 1.48102467, 1.48102467, 1.44781784,\n",
       "       1.42741935, 1.41129032, 1.41888046, 1.42979127, 1.40939279,\n",
       "       1.36527514, 1.30004744, 1.28344402, 1.24833966, 1.23410816,\n",
       "       1.21987666, 1.18287476, 1.1363852 , 1.13401328, 1.08847249,\n",
       "       1.07471537, 1.04245731, 1.01067362, 0.99027514, 0.94568311,\n",
       "       0.90393738, 0.90298861, 0.89065465, 0.86266603, 0.83420304,\n",
       "       0.80763757, 0.79577799, 0.77348197, 0.73885199, 0.71703036,\n",
       "       0.70042694, 0.68429791, 0.67291271, 0.65109108, 0.62073055,\n",
       "       0.60317837, 0.5823055 , 0.57685009, 0.55502846, 0.53273245,\n",
       "       0.5170778 , 0.50142315, 0.4743833 , 0.45588235, 0.43500949,\n",
       "       0.4259962 , 0.40986717, 0.39895636, 0.39041746, 0.38092979,\n",
       "       0.37001898, 0.36480076, 0.35104364, 0.33681214, 0.32305503,\n",
       "       0.3097723 , 0.2926945 , 0.28557875, 0.30313093, 0.30028463,\n",
       "       0.2983871 , 0.29459203, 0.29032258, 0.27324478, 0.26897533,\n",
       "       0.2585389 , 0.26470588, 0.2642315 , 0.27134725, 0.26043643,\n",
       "       0.2670778 , 0.27703985, 0.27466793, 0.27656546, 0.28036053,\n",
       "       0.28605313, 0.29459203, 0.29127135, 0.306926  , 0.306926  ,\n",
       "       0.31499051, 0.31641366, 0.30834915, 0.29981025, 0.30170778,\n",
       "       0.29411765, 0.29696395, 0.29981025, 0.3012334 , 0.30455408,\n",
       "       0.30645161, 0.30075901, 0.29506641, 0.30502846, 0.30550285,\n",
       "       0.3126186 , 0.31072106, 0.31214421, 0.31641366, 0.31119545,\n",
       "       0.31641366, 0.31593928, 0.30834915, 0.30265655, 0.30763757,\n",
       "       0.3066888 , 0.30763757, 0.30621442, 0.3066888 , 0.28961101,\n",
       "       0.29482922, 0.29862429, 0.29909867, 0.29577799, 0.29672676,\n",
       "       0.29625237, 0.30811195, 0.3066888 , 0.3009962 , 0.29767552,\n",
       "       0.28154649, 0.30147059, 0.30052182, 0.29672676, 0.28249526,\n",
       "       0.28344402, 0.28012334, 0.3038425 , 0.29340607, 0.30431689,\n",
       "       0.30526565, 0.29530361, 0.28581594, 0.27632827, 0.27917457,\n",
       "       0.26944972, 0.26565465, 0.26280835, 0.26043643, 0.27087287,\n",
       "       0.2670778 , 0.27988615, 0.26755218, 0.28036053, 0.31499051,\n",
       "       0.31925996, 0.32258065, 0.30882353, 0.30929791, 0.36053131,\n",
       "       0.38567362, 0.40322581, 0.39563567, 0.41034156, 0.41176471,\n",
       "       0.42409867, 0.45493359, 0.47343454, 0.46963947, 0.47248577,\n",
       "       0.46631879, 0.47343454, 0.471537  , 0.48149905, 0.4886148 ,\n",
       "       0.48671727, 0.4971537 , 0.50521822, 0.50521822, 0.53368121,\n",
       "       0.5227704 , 0.52371917, 0.50047438, 0.50332068, 0.48766603,\n",
       "       0.4601518 , 0.45351044, 0.45018975, 0.45777989, 0.45113852,\n",
       "       0.40749526, 0.37855787, 0.35673624, 0.33823529, 0.32447818,\n",
       "       0.33017078, 0.32352941, 0.31024668, 0.2926945 , 0.28510436,\n",
       "       0.28225806, 0.27609108, 0.25759013, 0.2613852 , 0.24810247,\n",
       "       0.2443074 , 0.24573055, 0.24810247, 0.24810247, 0.2471537 ,\n",
       "       0.21394687, 0.21489564, 0.22770398, 0.22912713, 0.22485769,\n",
       "       0.23671727, 0.23102467, 0.23434535, 0.24667932, 0.23719165,\n",
       "       0.23956357])"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "smoothed_averages['nat']['med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.57142857, 6.14285714, 5.71428571, 4.        , 5.        ,\n",
       "       6.        , 8.28571429, 7.85714286, 7.42857143, 7.        ])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_arr = np.insert(test_arr,0,test_arr[-3:])\n",
    "to_process = np.append(new_arr,test_arr[:3])\n",
    "movingaverage(to_process,7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Convert ndarrays to lists so they can be serialized/saved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,key in enumerate(statHolder.keys()):\n",
    "    if i == 0:\n",
    "        quantKeys = statHolder[key].keys()\n",
    "        for innerKey in quantKeys:\n",
    "            statHolder[key][innerKey] = statHolder[key][innerKey].tolist()\n",
    "            averages[key][innerKey] = averages[key][innerKey].tolist()\n",
    "            smoothed_averages[key][innerKey] = smoothed_averages[key][innerKey].tolist()\n",
    "            \n",
    "    elif i == 1:\n",
    "        states = statHolder[key].keys()\n",
    "        for state in states:\n",
    "            quantKeys = statHolder[key][state].keys()\n",
    "            for innerKey in quantKeys:\n",
    "                statHolder[key][state][innerKey] = statHolder[key][state][innerKey].tolist()\n",
    "                averages[key][state][innerKey] = averages[key][state][innerKey].tolist()\n",
    "                smoothed_averages[key][state][innerKey] = smoothed_averages[key][state][innerKey].tolist()\n",
    "                \n",
    "    else:\n",
    "        cwas = statHolder[key].keys()\n",
    "        for cwa in cwas:\n",
    "            quantKeys = statHolder[key][cwa].keys()\n",
    "            for innerKey in quantKeys:\n",
    "                statHolder[key][cwa][innerKey] = statHolder[key][cwa][innerKey].tolist()\n",
    "                averages[key][cwa][innerKey] = averages[key][cwa][innerKey].tolist()\n",
    "                smoothed_averages[key][cwa][innerKey] = smoothed_averages[key][cwa][innerKey].tolist()\n",
    "    \n",
    "    #print(statHolder[key].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2432., 19248., 16352., 10064., 37104.])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "#type(statHolder['nat']['ten'])\n",
    "\n",
    "with open('pow_climo_raw.json','w') as outfile:\n",
    "    json.dump(statHolder,outfile)\n",
    "    \n",
    "with open('pow_climo_avg.json','w') as outfile2:\n",
    "    json.dump(averages,outfile2)\n",
    "    \n",
    "with open('pow_climo_smAvg.json','w') as outfile3:\n",
    "    json.dump(smoothed_averages,outfile3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'../visualization/dashboard/backend/climo-data/pop_climo_raw.json') as file:\n",
    "    pop_data = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(a, n=3) :\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the # of tornado days (by dayIdx) for nat, each state, each CWA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTorDayHolder():\n",
    "    \n",
    "    with open('../visualization/exampleDashboard/includes/jsons/states.json') as json_file: \n",
    "        states = json.load(json_file)\n",
    "\n",
    "    with open('../visualization/exampleDashboard/includes/jsons/cwa.json') as json_file:\n",
    "        cwas = json.load(json_file)\n",
    "\n",
    "    statHolder = {'nat': np.zeros(366),'states': {}, 'cwas': {}}\n",
    "\n",
    "    for state in states:\n",
    "        statHolder['states'][state['abbreviation']] = np.zeros(366)\n",
    "\n",
    "    for cwa in cwas:\n",
    "        statHolder['cwas'][cwa['abbreviation']] = np.zeros(366)\n",
    "        \n",
    "        #print(cwa['abbreviation'])\n",
    "    return statHolder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "torDays = makeTorDayHolder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 file processing\n",
      "100 file processing\n",
      "200 file processing\n",
      "300 file processing\n",
      "400 file processing\n",
      "500 file processing\n",
      "600 file processing\n",
      "700 file processing\n",
      "800 file processing\n",
      "900 file processing\n",
      "1000 file processing\n",
      "1100 file processing\n",
      "1200 file processing\n",
      "1300 file processing\n",
      "1400 file processing\n",
      "1500 file processing\n",
      "1600 file processing\n",
      "1700 file processing\n",
      "1800 file processing\n",
      "1900 file processing\n",
      "2000 file processing\n",
      "2100 file processing\n",
      "2200 file processing\n",
      "2300 file processing\n",
      "2400 file processing\n",
      "2500 file processing\n",
      "2600 file processing\n",
      "2700 file processing\n",
      "2800 file processing\n",
      "2900 file processing\n",
      "3000 file processing\n",
      "3100 file processing\n",
      "3200 file processing\n",
      "3300 file processing\n",
      "3400 file processing\n",
      "3500 file processing\n",
      "3600 file processing\n",
      "3700 file processing\n",
      "3800 file processing\n",
      "3900 file processing\n",
      "4000 file processing\n",
      "4100 file processing\n",
      "4200 file processing\n",
      "4300 file processing\n",
      "4400 file processing\n",
      "4500 file processing\n",
      "4600 file processing\n",
      "4700 file processing\n",
      "4800 file processing\n",
      "4900 file processing\n",
      "5000 file processing\n",
      "5100 file processing\n",
      "5200 file processing\n",
      "5300 file processing\n",
      "5400 file processing\n",
      "5500 file processing\n",
      "5600 file processing\n",
      "5700 file processing\n",
      "5800 file processing\n",
      "5900 file processing\n",
      "6000 file processing\n",
      "6100 file processing\n",
      "6200 file processing\n",
      "6300 file processing\n",
      "6400 file processing\n",
      "6500 file processing\n",
      "6600 file processing\n",
      "6700 file processing\n",
      "6800 file processing\n",
      "6900 file processing\n",
      "7000 file processing\n",
      "7100 file processing\n",
      "7200 file processing\n",
      "7300 file processing\n",
      "7400 file processing\n",
      "7500 file processing\n",
      "7600 file processing\n",
      "7700 file processing\n",
      "7800 file processing\n",
      "7900 file processing\n",
      "8000 file processing\n",
      "8100 file processing\n",
      "8200 file processing\n",
      "8300 file processing\n",
      "8400 file processing\n",
      "8500 file processing\n",
      "8600 file processing\n",
      "8700 file processing\n",
      "8800 file processing\n",
      "8900 file processing\n",
      "9000 file processing\n",
      "9100 file processing\n",
      "9200 file processing\n",
      "9300 file processing\n",
      "9400 file processing\n",
      "9500 file processing\n",
      "9600 file processing\n",
      "9700 file processing\n",
      "9800 file processing\n",
      "9900 file processing\n",
      "10000 file processing\n",
      "10100 file processing\n",
      "10200 file processing\n",
      "10300 file processing\n",
      "10400 file processing\n",
      "10500 file processing\n",
      "10600 file processing\n",
      "10700 file processing\n",
      "10800 file processing\n",
      "10900 file processing\n",
      "11000 file processing\n",
      "11100 file processing\n"
     ]
    }
   ],
   "source": [
    "torDays = makeTorDayHolder()\n",
    "\n",
    "#impact = 'psubstations'\n",
    "\n",
    "for i,file in enumerate(files):\n",
    "    \n",
    "    if (i%100 == 0):\n",
    "        print(f'{i} file processing')\n",
    "    \n",
    "    # Get the index of the day and then subtract 1 for 0-indexing\n",
    "    date = datetime.datetime.strptime(file.split('/')[-1].split('_')[0][:-4],'%Y%m%d')\n",
    "    dayIdx = int(date.strftime('%j'))-1\n",
    "    # If not a leap year and the dayIdx is beyond what would be the leap day, add 1 to the dayIdx\n",
    "    if ((date.year%4 != 0) and (dayIdx > 58)):\n",
    "        dayIdx += 1\n",
    "    \n",
    "    # Set up national stats\n",
    "    df = pd.read_csv(file, sep=\"|\")\n",
    "    '''sims = df.groupby(\"sim\")\n",
    "    fields = sims.sum().loc[:,('population','hospitals','mobilehomes','psubstations')]\n",
    "    # Fill missing sims with 0s\n",
    "    fill_fields = fields.reindex(list(range(1,10001)),fill_value=0)\n",
    "    \n",
    "    # Get national quantiles for this file\n",
    "    quants = fields.describe(percentiles=[0.1,0.5,0.9]).loc[('min','10%','50%','90%','max'),[impact]][impact].values.tolist()\n",
    "    \n",
    "    # Add quantiles to the proper slot in the national sub-dictionary\n",
    "    statHolder['nat']['min'][int(dayIdx)] += quants[0]\n",
    "    statHolder['nat']['ten'][int(dayIdx)] += quants[1]\n",
    "    statHolder['nat']['med'][int(dayIdx)] += quants[2]\n",
    "    statHolder['nat']['ninety'][int(dayIdx)] += quants[3]\n",
    "    statHolder['nat']['max'][int(dayIdx)] += quants[4] '''\n",
    "    \n",
    "    \n",
    "    torDays['nat'][int(dayIdx)] += 1\n",
    "    \n",
    "    \n",
    "    ##### States #####\n",
    "    \n",
    "    stBrokenOut = df.assign(category=df['states'].str.split(',')).explode('category').reset_index(drop=True)\n",
    "\n",
    "    # Remove row if state is NaN\n",
    "    stBrokenOut = stBrokenOut[stBrokenOut['category'].notna()]\n",
    "\n",
    "    # Grab a list of the unique states in the simulation\n",
    "    statesImpacted = stBrokenOut['category'].unique().tolist()\n",
    "    \n",
    "    # Loop through impacted states\n",
    "    for state in statesImpacted:\n",
    "        \n",
    "        torDays['states'][state][int(dayIdx)] += 1\n",
    "\n",
    "        # Organize dataframe for state and fill no-impact sims with 0s\n",
    "        '''ind_state = stBrokenOut[stBrokenOut['category'] == state]\n",
    "        #print(f'State: {state}')\n",
    "        ind_state_group = ind_state.groupby(\"sim\").sum().loc[:,['population','hospitals','mobilehomes','psubstations']]\n",
    "        ind_state_tot = ind_state_group.reindex(list(range(1,10001)),fill_value=0)\n",
    "        \n",
    "        # Get state quantiles for this file\n",
    "        quants = ind_state_tot.describe(percentiles=[0.1,0.5,0.9]).loc[['min','10%','50%','90%','max'],[impact]][impact].values.tolist()\n",
    "        \n",
    "        # Add quantiles to the proper slot in the national sub-dictionary\n",
    "        statHolder['states'][state]['min'][int(dayIdx)] += quants[0]\n",
    "        statHolder['states'][state]['ten'][int(dayIdx)] += quants[1]\n",
    "        statHolder['states'][state]['med'][int(dayIdx)] += quants[2]\n",
    "        statHolder['states'][state]['ninety'][int(dayIdx)] += quants[3]\n",
    "        statHolder['states'][state]['max'][int(dayIdx)] += quants[4]'''\n",
    "        \n",
    "    ##### CWAs #####\n",
    "    \n",
    "    cwaBrokenOut = df.assign(category=df['wfos'].str.split(',')).explode('category').reset_index(drop=True)\n",
    "\n",
    "    # Remove row if state is NaN\n",
    "    cwaBrokenOut = cwaBrokenOut[cwaBrokenOut['category'].notna()]\n",
    "\n",
    "    # Grab a list of the unique states in the simulation\n",
    "    cwasImpacted = cwaBrokenOut['category'].unique().tolist()\n",
    "    \n",
    "    # Loop through impacted states\n",
    "    for cwa in cwasImpacted:\n",
    "        \n",
    "        torDays['cwas'][cwa][int(dayIdx)] += 1\n",
    "\n",
    "        # Organize dataframe for state and fill no-impact sims with 0s\n",
    "        '''ind_cwa = cwaBrokenOut[cwaBrokenOut['category'] == cwa]\n",
    "        #print(f'State: {state}')\n",
    "        ind_cwa_group = ind_cwa.groupby(\"sim\").sum().loc[:,['population','hospitals','mobilehomes','psubstations']]\n",
    "        ind_cwa_tot = ind_cwa_group.reindex(list(range(1,10001)),fill_value=0)\n",
    "        \n",
    "        # Get state quantiles for this file\n",
    "        quants = ind_cwa_tot.describe(percentiles=[0.1,0.5,0.9]).loc[['min','10%','50%','90%','max'],[impact]][impact].values.tolist()\n",
    "        \n",
    "        try:\n",
    "            # Add quantiles to the proper slot in the national sub-dictionary\n",
    "            statHolder['cwas'][cwa]['min'][int(dayIdx)] += quants[0]\n",
    "            statHolder['cwas'][cwa]['ten'][int(dayIdx)] += quants[1]\n",
    "            statHolder['cwas'][cwa]['med'][int(dayIdx)] += quants[2]\n",
    "            statHolder['cwas'][cwa]['ninety'][int(dayIdx)] += quants[3]\n",
    "            statHolder['cwas'][cwa]['max'][int(dayIdx)] += quants[4]\n",
    "        except:\n",
    "            print(file)'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert np arrays of tor days to lists for saving"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,key in enumerate(torDays.keys()):\n",
    "    if i == 0:\n",
    "        torDays['nat'] = torDays['nat'].tolist()\n",
    "    elif i == 1:\n",
    "        states = torDays['states'].keys()\n",
    "        for state in states:\n",
    "            torDays['states'][state] = torDays['states'][state].tolist()\n",
    "    elif i == 2:\n",
    "        cwas = torDays['cwas'].keys()\n",
    "        for cwa in cwas:\n",
    "            torDays['cwas'][cwa] = torDays['cwas'][cwa].tolist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tor_days.json','w') as outfile:\n",
    "    json.dump(torDays,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('tor_days.json','r') as tors:\n",
    "    torlist = json.load(tors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create moving averages using only tor days"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Read in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../climoFiles/hosp_climo_raw.json','r') as hosp:\n",
    "    hosplist = json.load(hosp)\n",
    "\n",
    "with open('../climoFiles/tor_days.json','r') as tor:\n",
    "    torlist = json.load(tor)\n",
    "    \n",
    "with open('../climoFiles/mob_climo_raw.json','r') as mob:\n",
    "    moblist = json.load(mob)\n",
    "    \n",
    "with open('../climoFiles/pow_climo_raw.json','r') as pow:\n",
    "    powlist = json.load(pow)\n",
    "    \n",
    "with open('../climoFiles/pop_climo_raw.json','r') as pop:\n",
    "    poplist = json.load(pop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "window = 31\n",
    "halfwin = math.floor(window/2)\n",
    "\n",
    "rando = torlist['nat']\n",
    "rando = np.array(rando)\n",
    "rando1 = np.insert(rando,0,rando[-halfwin:])\n",
    "rando2 = np.append(rando1,rando[:halfwin])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function adds values to the beginning and end to make it ready for summing\n",
    "def arrExtender(array,window):\n",
    "    halfwin = math.floor(window/2)\n",
    "    \n",
    "    rando = array\n",
    "    rando = np.array(rando)\n",
    "    rando1 = np.insert(rando,0,rando[-halfwin:])\n",
    "    rando2 = np.append(rando1,rando[:halfwin])\n",
    "    \n",
    "    return rando2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summer(array,window):\n",
    "\n",
    "    total = np.array([])\n",
    "\n",
    "    start = 0\n",
    "    end = window\n",
    "    while end < (len(array)+1):\n",
    "        total = np.append(total,np.sum(array[start:end]))\n",
    "        start += 1\n",
    "        end += 1\n",
    "        \n",
    "    return total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([11.93706294, 11.95818815, 11.89690722, 11.9       , 11.9966443 ,\n",
       "       12.15511551, 12.45394737, 12.2640264 , 12.29666667, 12.29470199,\n",
       "       12.27574751, 12.21594684, 12.34437086, 12.3496732 , 12.26885246,\n",
       "       12.24503311, 12.10223642, 11.8369906 , 11.75157233, 11.73650794,\n",
       "       11.90822785, 12.        , 12.03236246, 12.05177994, 12.27741935,\n",
       "       12.17088608, 12.2866242 , 12.36624204, 12.37539432, 12.38679245,\n",
       "       12.30091185, 12.35311573, 12.38529412, 12.41107872, 12.38616715,\n",
       "       12.46685879, 12.36723164, 12.19832402, 12.21823204, 12.33150685,\n",
       "       12.40108401, 12.48773842, 12.45576408, 12.84168865, 12.82841823,\n",
       "       12.8381201 , 12.92620865, 13.05012531, 13.19506173, 13.23021583,\n",
       "       13.33018868, 13.08863636, 13.08849558, 13.09230769, 13.16198704,\n",
       "       12.94092827, 13.02057613, 12.95219124, 12.80813953, 12.71374046,\n",
       "       12.9738806 , 12.96296296, 12.96402878, 12.81643357, 12.99136442,\n",
       "       12.9862543 , 12.87945671, 12.84317032, 12.79900332, 12.78360656,\n",
       "       12.75641026, 12.75077882, 12.79939668, 12.72238372, 12.47666195,\n",
       "       12.43852459, 12.49662618, 12.61986755, 12.87386216, 12.84122919,\n",
       "       12.94836272, 12.8925    , 12.98019802, 12.97688564, 12.96300716,\n",
       "       13.08018868, 13.1087963 , 13.08870968, 13.07457627, 13.20467186,\n",
       "       13.26564215, 13.15743757, 13.15031983, 13.15522703, 13.43995859,\n",
       "       13.29191919, 13.28301887, 13.32623427, 13.39005736, 13.42910798,\n",
       "       13.46210721, 13.53691887, 13.51215122, 13.54553492, 13.66901408,\n",
       "       13.65800866, 13.68434559, 13.59531773, 13.48181818, 13.3388023 ,\n",
       "       13.35720098, 13.29581994, 13.30298273, 13.30046225, 13.37642586,\n",
       "       13.42304833, 13.37518248, 13.39086294, 13.41935484, 13.48224432,\n",
       "       13.4395218 , 13.42222222, 13.31442242, 13.31781377, 13.2738811 ,\n",
       "       13.1152565 , 13.05640345, 12.99212598, 12.91828794, 12.86917677,\n",
       "       12.93860759, 12.94051346, 12.85971446, 12.85326757, 12.80981595,\n",
       "       12.76909091, 12.78769602, 12.71017964, 12.74014337, 12.72818991,\n",
       "       12.67059517, 12.6532634 , 12.64678633, 12.63076037, 12.6228473 ,\n",
       "       12.53092784, 12.432     , 12.36259977, 12.37068478, 12.34194273,\n",
       "       12.20324748, 12.17112598, 12.16417083, 12.15133038, 12.09723757,\n",
       "       12.05778756, 12.02349727, 12.05211726, 12.12216216, 12.11388286,\n",
       "       12.09303591, 11.95424837, 11.89934712, 11.83297062, 11.84636413,\n",
       "       11.77704918, 11.74862788, 11.67436744, 11.6521978 , 11.56569544,\n",
       "       11.51894563, 11.49587232, 11.44137168, 11.33997785, 11.25013866,\n",
       "       11.14991671, 11.10962716, 11.05313199, 11.02633053, 10.93243243,\n",
       "       10.91704289, 10.95081967, 10.86541738, 10.74985722, 10.71592211,\n",
       "       10.67951669, 10.71288273, 10.7260592 , 10.67231308, 10.57453052,\n",
       "       10.54225352, 10.48091603, 10.50618011, 10.47751479, 10.47797619,\n",
       "       10.38347206, 10.36091127, 10.25997582, 10.22968845, 10.20257827,\n",
       "       10.20812808, 10.16728856, 10.09830933, 10.02071563, 10.017067  ,\n",
       "       10.01464036, 10.0357599 , 10.10705128, 10.11411992, 10.0926766 ,\n",
       "       10.09622887, 10.01707157,  9.98543046,  9.98076923, 10.03118779,\n",
       "       10.04180491, 10.0348292 ,  9.95836132,  9.86351351,  9.82401091,\n",
       "        9.74465148,  9.74058577,  9.68736768,  9.63894812,  9.5793252 ,\n",
       "        9.56014493,  9.62618182,  9.66055718,  9.71868132,  9.72626932,\n",
       "        9.72551929,  9.77252252,  9.79409538,  9.89778795,  9.98767334,\n",
       "       10.01549187,  9.99376461,  9.92575039,  9.813749  ,  9.86882591,\n",
       "        9.85714286,  9.84507042,  9.85547201,  9.82502113,  9.86844368,\n",
       "        9.84247171,  9.83348096,  9.88361683,  9.95081967,  9.96497696,\n",
       "       10.01386322, 10.0724365 , 10.084372  , 10.11304348, 10.16157635,\n",
       "       10.25275827, 10.29137056, 10.23260644, 10.15303983, 10.12258065,\n",
       "       10.09269357, 10.17647059, 10.1131019 , 10.11199095, 10.05727377,\n",
       "       10.02217036, 10.08392435, 10.13559322, 10.21588089, 10.25158028,\n",
       "       10.26181354, 10.34464752, 10.41810919, 10.44864865, 10.58333333,\n",
       "       10.65048544, 10.85572843, 11.01310044, 10.93452381, 10.92857143,\n",
       "       11.00622084, 11.0798722 , 11.19743178, 11.1503268 , 11.33445378,\n",
       "       11.31292517, 11.35060449, 11.31348511, 11.34046346, 11.46376812,\n",
       "       11.54014599, 11.58441558, 11.60377358, 11.69573643, 11.8151751 ,\n",
       "       11.86345382, 11.99591837, 12.0464135 , 12.14767932, 12.23319328,\n",
       "       12.25265393, 12.29677419, 12.24449339, 12.10352423, 12.42888889,\n",
       "       12.5258427 , 12.64920273, 12.5372093 , 12.56689342, 12.70938215,\n",
       "       12.73549884, 13.01631702, 13.14218009, 13.20048309, 13.35365854,\n",
       "       13.34390244, 13.34146341, 13.21515892, 13.24754902, 13.27339901,\n",
       "       13.28140704, 13.31060606, 13.37939698, 13.30150754, 13.57035176,\n",
       "       13.47179487, 13.46153846, 13.33247423, 13.17783505, 13.18302387,\n",
       "       13.13513514, 13.26702997, 13.21487603, 13.29752066, 13.38700565,\n",
       "       13.09883721, 12.94047619, 12.86363636, 12.83333333, 12.56782334,\n",
       "       12.58544304, 12.43396226, 12.27301587, 12.1163522 , 12.06624606,\n",
       "       11.97204969, 11.97476341, 11.94822006, 12.06840391, 12.13666667,\n",
       "       12.15824916, 12.17845118, 12.31506849, 12.32525952, 12.41608392,\n",
       "       12.02877698, 11.97132616, 11.97894737, 11.95470383, 12.10839161,\n",
       "       12.15277778, 12.15679443, 12.02768166, 12.03092784, 11.97923875,\n",
       "       12.03819444])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# This is the 31-day running sum (centered on the middle date)\n",
    "summer(arrExtender(hosplist['nat']['max'],window),window)/summer(arrExtender(torlist['nat'],window),window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/josephpicca/anaconda/envs/impacts/lib/python3.7/site-packages/ipykernel_launcher.py:25: RuntimeWarning: invalid value encountered in true_divide\n",
      "/Users/josephpicca/anaconda/envs/impacts/lib/python3.7/site-packages/ipykernel_launcher.py:39: RuntimeWarning: invalid value encountered in true_divide\n"
     ]
    }
   ],
   "source": [
    "torDayStats = makeStatHolder()\n",
    "impactList = poplist\n",
    "\n",
    "for i,key in enumerate(torDayStats.keys()):\n",
    "    if i == 0:\n",
    "        \n",
    "        torDays = summer(arrExtender(torlist['nat'],window),window)\n",
    "        \n",
    "        quantKeys = torDayStats[key].keys()\n",
    "        for innerKey in quantKeys:\n",
    "            # Create list of daily 31-day averages\n",
    "            torDayStats[key][innerKey] = (summer(arrExtender(impactList[key][innerKey],window),window)/torDays).tolist()\n",
    "         \n",
    "    elif i == 1:\n",
    "        \n",
    "        states = torDayStats[key].keys()\n",
    "        for state in states:\n",
    "            \n",
    "            # Get the tor days for individual states\n",
    "            torDays = summer(arrExtender(torlist['states'][state],window),window)\n",
    "        \n",
    "        #quantKeys = torDayStats[key].keys()\n",
    "            for innerKey in quantKeys:\n",
    "                # Create list of daily 31-day averages\n",
    "                torDayStats[key][state][innerKey] = (summer(arrExtender(impactList[key][state][innerKey],window),window)/torDays).tolist()\n",
    "\n",
    "              \n",
    "    else:\n",
    "        \n",
    "        cwas = torDayStats[key].keys()\n",
    "        for cwa in cwas:\n",
    "            \n",
    "            # Get the tor days for individual CWAs\n",
    "            torDays = summer(arrExtender(torlist['cwas'][cwa],window),window)\n",
    "        \n",
    "        #quantKeys = torDayStats[key].keys()\n",
    "            for innerKey in quantKeys:\n",
    "                # Create list of daily 31-day averages\n",
    "                torDayStats[key][cwa][innerKey] = (summer(arrExtender(impactList[key][cwa][innerKey],window),window)/torDays).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../climoFiles/pop_climo_torSmAvg.json','w') as outfile:\n",
    "    json.dump(torDayStats,outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../climoFiles/pop_climo_torSmAvg.json','r') as outfile:\n",
    "    test = json.load(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "98.39481437831468"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test['nat']['min'][140]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
