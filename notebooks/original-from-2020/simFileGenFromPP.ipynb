{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "mport pygrib as pg\n",
    "import pathlib\n",
    "import numpy as np\n",
    "\n",
    "import pathlib\n",
    "import glob\n",
    "import os\n",
    "import datetime\n",
    "import pyproj\n",
    "from scipy import stats\n",
    "import gzip\n",
    "\n",
    "import dataclasses as dc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initial Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ndfd_area = 25\n",
    "nsims = 10000\n",
    "tornado_direction_distribution = stats.norm(50,15)\n",
    "coolseason = [1,2,3,4,11,12]\n",
    "\n",
    "# Impacts environment variable & root\n",
    "os.environ['IMPACTSDATA'] = \"../impacts-data\"\n",
    "impacts_data_root = pathlib.Path(os.environ['IMPACTSDATA'],\"pas-input-data\").expanduser().resolve()\n",
    "\n",
    "# pp forecast file list\n",
    "pp_reg_files = glob.glob('./pp_forecasts/regProbs/*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to read pp file and format it properly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Volumes/Backup Plus/pas_output/simulationFiles/*'\n",
    "paths = sorted(glob.glob(filepath), key=os.path.getmtime)\n",
    "\n",
    "lastIdx = paths.index('/Volumes/Backup Plus/pas_output/simulationFiles/195001031630_pp.psv.gz') + 1\n",
    "#toRun = np.arange(0,lastIdx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Backup Plus/pas_output/simulationFiles/199812051630_pp.psv.gz'"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Volumes/Backup Plus/pas_output/simulationFiles/195001031630_pp.psv.gz'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths[lastIdx-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_pp_file(file):\n",
    "    \"\"\" Read a PP file and format properly for the PAS \"\"\"\n",
    "    data = np.load(file)\n",
    "    probs = data['fcst']*100\n",
    "    vals = np.where(probs==0, -1.,probs)\n",
    "    \n",
    "    return vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Output directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "outdir = pathlib.Path(\"/\",\"Volumes\",\"Backup Plus\",\"pas_output\",\"simulationFiles\").resolve()\n",
    "#outdir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through PP files and write out simulation files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running file #0\n"
     ]
    }
   ],
   "source": [
    "# Updated to only run sim on older dates that need re-running\n",
    "for i, string in enumerate(paths[3043:3045]): \n",
    "#for i, string in enumerate(paths):\n",
    "\n",
    "    \n",
    "    if (i % 100 == 0): print(f'Running file #{i}');\n",
    "        \n",
    "    ### Get actual file by crosschecking the already finished simulations\n",
    "    newDate = string.split('/')[-1].split('1630')[0]\n",
    "    file = glob.glob(f'./pp_forecasts/regProbs/{newDate}*')[0]\n",
    "    #print(file)\n",
    "    \n",
    "    ### Setup Simulation ###\n",
    "    # Determine Date/Time of outlook from filename\n",
    "    date_in_name = file.split(\"/\")[-1].split(\".\")[0]\n",
    "    dt = datetime.datetime.strptime(date_in_name, \"%Y%m%d_%H%M\")\n",
    "    outfile = outdir.joinpath(f\"{dt.strftime('%Y%m%d%H%M')}_pp_new.psv.gz\")\n",
    "    \n",
    "    # Read file\n",
    "    torn = read_pp_file(file)\n",
    "    \n",
    "    # Make continuous probs\n",
    "    try:\n",
    "        continuous_torn = dc.make_continuous(torn)\n",
    "    except ValueError:\n",
    "        import sys\n",
    "        if torn.max() == 0:\n",
    "            with gzip.GzipFile(outfile, \"w\") as OUT:\n",
    "                OUT.write(\"\".encode())\n",
    "            sys.exit(0)\n",
    "        else:\n",
    "            print(\"There was an uncaught error converting to continuous probabilities. Skipping file...\")\n",
    "            continue\n",
    "            #sys.exit(1)\n",
    "    \n",
    "    if (glob.glob(f'./pp_forecasts/sigProbs/{date_in_name}.npz')):\n",
    "        \n",
    "        sigtorn = read_pp_file(f'./pp_forecasts/sigProbs/{date_in_name}.npz').astype(int)\n",
    "        \n",
    "        # Need this line to format sig torn file to work properly with logic (-1s replaced with 0s)\n",
    "        sigtorn[sigtorn == -1] = 0\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        # If there is no PP sig file, then create a 2-d array of 0s\n",
    "        sigtorn = np.ndarray(shape=np.shape(torn), dtype=int, order='F')\n",
    "        \n",
    "    # Where there are 10 sig tor probs, set to 1\n",
    "    sigtorn[sigtorn > 0] = 1\n",
    "    \n",
    "    # Make double sig (if necessary)\n",
    "    if (torn.max() >= 30) and (sigtorn.max() > 0):\n",
    "        sigtorn[torn >= 15] += 1\n",
    "    \n",
    "    # usesig changes the distribution used for tornado sampling\n",
    "    sigtorn_1d = sigtorn.ravel()\n",
    "    usesig = True if (dt.month in coolseason) or (sigtorn.max() > 0) else False\n",
    "    \n",
    "    ### Run Tornado Count Simulation ###\n",
    "    #print(f\"Running {nsims:,d} Tornado Count Simulations\")\n",
    "    tornado_dists = dc.TornadoDistributions()\n",
    "    counts = np.zeros((5, nsims), dtype=int)\n",
    "    counts[0, :] = (tornado_dists.f02.rvs(nsims) * ndfd_area * (torn == 2).sum()).astype(int)\n",
    "    counts[1, :] = (tornado_dists.f05.rvs(nsims) * ndfd_area * (torn == 5).sum()).astype(int)\n",
    "    counts[2, :] = (tornado_dists.f10.rvs(nsims) * ndfd_area * (torn == 10).sum()).astype(int)\n",
    "    counts[3, :] = (tornado_dists.f15.rvs(nsims) * ndfd_area * (torn == 15).sum()).astype(int)\n",
    "    counts[4, :] = (tornado_dists.f30.rvs(nsims) * ndfd_area * (torn >= 30).sum()).astype(int)\n",
    "    \n",
    "    ### Setup Impact Simulation ###\n",
    "    igrids = dc.ImpactGrids(impacts_data_root)\n",
    "\n",
    "    # Determine the indices of tornadoes for each prob level\n",
    "    scounts = counts.sum(axis=1)\n",
    "    inds02 = dc.weighted_choice(prob=2, probs=torn, cprobs=continuous_torn, size=scounts[0])\n",
    "    inds05 = dc.weighted_choice(prob=5, probs=torn, cprobs=continuous_torn, size=scounts[1])\n",
    "    inds10 = dc.weighted_choice(prob=10, probs=torn, cprobs=continuous_torn, size=scounts[2])\n",
    "    inds15 = dc.weighted_choice(prob=15, probs=torn, cprobs=continuous_torn, size=scounts[3])\n",
    "    inds30 = dc.weighted_choice(prob=30, probs=torn, cprobs=continuous_torn, size=scounts[4])\n",
    "    inds = dc.flatten_list([inds02, inds05, inds10, inds15, inds30])\n",
    "    \n",
    "    non_sig_inds = sigtorn_1d[inds] == 0\n",
    "    single_sig_inds = sigtorn_1d[inds] == 1\n",
    "    double_sig_inds = sigtorn_1d[inds] == 2\n",
    "\n",
    "    if usesig:\n",
    "        single_sig_inds += non_sig_inds\n",
    "        non_sig_inds[:] = False\n",
    "        \n",
    "    # Handle Locations\n",
    "    non_sig_loc_inds = inds[non_sig_inds]\n",
    "    single_sig_loc_inds = inds[single_sig_inds]\n",
    "    double_sig_loc_inds = inds[double_sig_inds]\n",
    "    \n",
    "    # Handle Ratings\n",
    "    _mags=[0, 1, 2, 3, 4, 5]\n",
    "    non_sig_ratings = np.random.choice(_mags, size=non_sig_inds.sum(),\n",
    "                                       replace=True, p=tornado_dists.r_nonsig)\n",
    "    single_sig_ratings = np.random.choice(_mags, size=single_sig_inds.sum(),\n",
    "                                          replace=True, p=tornado_dists.r_singlesig)\n",
    "    double_sig_ratings = np.random.choice(_mags, size=double_sig_inds.sum(),\n",
    "                                          replace=True, p=tornado_dists.r_doublesig)\n",
    "    \n",
    "    # Handle Distances\n",
    "    non_sig_distances = dc.get_distances(non_sig_ratings, tornado_dists)\n",
    "    single_sig_distances = dc.get_distances(single_sig_ratings, tornado_dists)\n",
    "    double_sig_distances = dc.get_distances(double_sig_ratings, tornado_dists)\n",
    "    \n",
    "    #print(\"Running simulations...\")\n",
    "    #print(\"    Non Sig...\")\n",
    "    non_sig = dc.simulate(non_sig_loc_inds, non_sig_distances,\n",
    "                          non_sig_ratings, tornado_direction_distribution, igrids)\n",
    "    #print(\"    Single Sig...\")\n",
    "    single_sig = dc.simulate(single_sig_loc_inds, single_sig_distances,\n",
    "                             single_sig_ratings, tornado_direction_distribution, igrids)\n",
    "    #print(\"    Double Sig...\")\n",
    "    double_sig = dc.simulate(double_sig_loc_inds, double_sig_distances,\n",
    "                             double_sig_ratings, tornado_direction_distribution, igrids)\n",
    "\n",
    "    #print(\"Splitting simulations back out...\")\n",
    "    simulated_tornadoes = dc.flatten_list([non_sig, single_sig, double_sig])\n",
    "    np.random.shuffle(simulated_tornadoes)\n",
    "    _sims = np.split(simulated_tornadoes, counts.sum(axis=0).cumsum())[:-1]\n",
    "    realizations = dc.Realizations([dc.SyntheticTornadoRealization(_sim, i+1) for i, _sim in enumerate(_sims)])\n",
    "\n",
    "\n",
    "    #print(\"Writing Out gzipped PSV file...\")\n",
    "    with gzip.GzipFile(outfile, \"w\") as OUT:\n",
    "        OUT.write(realizations.as_psv.encode())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Files with errors..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Errors are due to arrays without probs (due to the one tor file having a day with a tornado in PR or somewhere else)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3044"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "paths.index('/Volumes/Backup Plus/pas_output/simulationFiles/199104091630_pp.psv.gz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Backup Plus/pas_output/simulationFiles/199104091630_pp.psv.gz\n"
     ]
    }
   ],
   "source": [
    "for i, string in enumerate(paths[3044:3045]):\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting bad files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## To do once sims are done\n",
    "- Run through all files, read in data to df and inspect the county column -- if any tor row has greater than ~4 counties, add the file name to a list for further inspection\n",
    "- Re-run sims on 19830607\n",
    "- Re-run sims on 19990601\n",
    "- 20110427"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = '/Volumes/Backup Plus/pas_output/simulationFiles/*'\n",
    "paths = glob.glob(filepath)\n",
    "\n",
    "to_check = naughtyList\n",
    "naughtyList = []\n",
    "\n",
    "for path in to_check:\n",
    "    df = pd.read_csv(path,sep='|',nrows=50)\n",
    "    \n",
    "    cos_impacted = df['counties'].apply(lambda x: len(str(x).split(','))).mean()\n",
    "    \n",
    "    if cos_impacted > 2:\n",
    "        #print('Uh oh!')\n",
    "        naughtyList.append(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['/Volumes/Backup Plus/pas_output/simulationFiles/199812051630_pp.psv.gz']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "naughtyList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naughtylist.txt', 'w') as filehandle:\n",
    "    for listitem in naughtyList:\n",
    "        filehandle.write(f'{listitem}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('naughtylist.txt') as f:\n",
    "    naughtyList = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = np.ma.array([1,2,3,4,5], mask=[False,False,True,True,False])\n",
    "b = np.array([1,2,np.nan,np.nan,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.5, nan, nan, nan])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.convolve(b,[0.5,0.5],'valid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = np.insert(b,0,b[4])\n",
    "d = np.append(c,b[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = 0\n",
    "end = 3\n",
    "mov_avg = []\n",
    "\n",
    "while (end < (len(d)+1)):\n",
    "    \n",
    "    to_avg = d[start:end]\n",
    "    nonzero_count = np.count_nonzero(~np.isnan(to_avg))\n",
    "    total = np.sum(np.nan_to_num(to_avg))\n",
    "    mov_avg.append(total/nonzero_count)\n",
    "    \n",
    "    start += 1\n",
    "    end += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.6666666666666665, 1.5, 2.0, 5.0, 3.0]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mov_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero(~np.isnan(d))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.count_nonzero([0,1,0,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
