{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T17:08:20.986071Z",
     "iopub.status.busy": "2021-08-05T17:08:20.985779Z",
     "iopub.status.idle": "2021-08-05T17:08:22.935538Z",
     "shell.execute_reply": "2021-08-05T17:08:22.934725Z",
     "shell.execute_reply.started": "2021-08-05T17:08:20.986004Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from pygridder import pygridder as pgrid\n",
    "import pyproj\n",
    "import pathlib\n",
    "\n",
    "import multiprocessing.popen_spawn_posix\n",
    "import dask\n",
    "from dask.distributed import Client, progress\n",
    "\n",
    "import skimage.morphology as skmorph\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import scipy.ndimage as ndimage\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T17:08:25.923275Z",
     "iopub.status.busy": "2021-08-05T17:08:25.923058Z",
     "iopub.status.idle": "2021-08-05T17:08:25.926353Z",
     "shell.execute_reply": "2021-08-05T17:08:25.925629Z",
     "shell.execute_reply.started": "2021-08-05T17:08:25.923251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T17:08:26.577311Z",
     "iopub.status.busy": "2021-08-05T17:08:26.577061Z",
     "iopub.status.idle": "2021-08-05T17:08:26.737770Z",
     "shell.execute_reply": "2021-08-05T17:08:26.737302Z",
     "shell.execute_reply.started": "2021-08-05T17:08:26.577283Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pygrib as pg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T17:08:27.327190Z",
     "iopub.status.busy": "2021-08-05T17:08:27.326966Z",
     "iopub.status.idle": "2021-08-05T17:08:27.335018Z",
     "shell.execute_reply": "2021-08-05T17:08:27.334472Z",
     "shell.execute_reply.started": "2021-08-05T17:08:27.327165Z"
    }
   },
   "outputs": [],
   "source": [
    "import dclasses as dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-06T15:09:01.040829Z",
     "iopub.status.busy": "2021-08-06T15:09:01.040599Z",
     "iopub.status.idle": "2021-08-06T15:09:02.077130Z",
     "shell.execute_reply": "2021-08-06T15:09:02.076416Z",
     "shell.execute_reply.started": "2021-08-06T15:09:01.040804Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = Client(n_workers=2,threads_per_worker=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-06T19:18:10.652818Z",
     "iopub.status.busy": "2021-08-06T19:18:10.652469Z",
     "iopub.status.idle": "2021-08-06T19:18:11.185529Z",
     "shell.execute_reply": "2021-08-06T19:18:11.184165Z",
     "shell.execute_reply.started": "2021-08-06T19:18:10.652791Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "distributed.client - ERROR - Failed to reconnect to scheduler after 10.00 seconds, closing client\n",
      "_GatheringFuture exception was never retrieved\n",
      "future: <_GatheringFuture finished exception=CancelledError()>\n",
      "asyncio.exceptions.CancelledError\n"
     ]
    }
   ],
   "source": [
    "client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T17:32:13.567958Z",
     "iopub.status.busy": "2021-08-05T17:32:13.567733Z",
     "iopub.status.idle": "2021-08-05T17:32:15.307825Z",
     "shell.execute_reply": "2021-08-05T17:32:15.307211Z",
     "shell.execute_reply.started": "2021-08-05T17:32:13.567933Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Global Variables & Pre-Processing for PP forecast\n",
    "dx = 5 # delta x\n",
    "selem = skmorph.disk(40 / dx) # morphology disk\n",
    "\n",
    "ndfd_path = pathlib.Path('../scripts/pas-input-data/ndfd.npz').resolve()\n",
    "with np.load(ndfd_path) as NPZ:\n",
    "    lons = NPZ['lons']\n",
    "    lats = NPZ['lats']\n",
    "    \n",
    "G = pgrid.Gridder(tx=lons, ty=lats, dx=dx/100)\n",
    "\n",
    "dateparser = lambda x: dt.datetime.strptime(x, '%Y-%m-%d %H:%M:%S') + dt.timedelta(hours=6)\n",
    "data_path = pathlib.Path('../../../gen-assets/1950-2019_tors_CONUS.csv')\n",
    "\n",
    "df = pd.read_csv(data_path, parse_dates=[['date','time']], date_parser=dateparser, index_col=0, keep_date_col=True)\n",
    "df = df.reset_index()\n",
    "\n",
    "all_di = {0: 0.0, 1: 1.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0}\n",
    "sig_di = {0: 0.0, 1: 0.0, 2: 1.0, 3: 1.0, 4: 1.0, 5: 1.0}\n",
    "df['all_weight'] = df['mag'].map(all_di).fillna(df['mag'])\n",
    "df['sig_weight'] = df['mag'].map(sig_di).fillna(df['mag'])\n",
    "\n",
    "outlook_time = '1200'\n",
    "\n",
    "# Create date array\n",
    "my_hour = dt.datetime.strptime(outlook_time, '%H%M').hour\n",
    "my_minute = dt.datetime.strptime(outlook_time, '%H%M').minute\n",
    "\n",
    "bdt = dt.datetime(1950,1,1,my_hour,my_minute)\n",
    "edt = dt.datetime(2019,12,31,my_hour,my_minute)\n",
    "\n",
    "# Create list index of datetimes with a frequency of one per day\n",
    "dts = pd.date_range(bdt, edt, freq='D')\n",
    "bdts, edts = dts[:-1],dts[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T17:32:17.679356Z",
     "iopub.status.busy": "2021-08-05T17:32:17.679132Z",
     "iopub.status.idle": "2021-08-05T17:32:17.685567Z",
     "shell.execute_reply": "2021-08-05T17:32:17.684881Z",
     "shell.execute_reply.started": "2021-08-05T17:32:17.679332Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# PAS Global Variables and Pre-Processing\n",
    "### Parse CLI Arguments ###\n",
    "\n",
    "ndfd_area = 25\n",
    "nsims = 10000\n",
    "tornado_direction_distribution = stats.norm(50, 15)\n",
    "coolseason = [1, 2, 3, 4, 11, 12]\n",
    "\n",
    "impacts_data_root = pathlib.Path('../scripts/pas-input-data/')\n",
    "outdir = pathlib.Path('./PAS-climo/',\"output\").resolve()\n",
    "outdir.mkdir(exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "## Baseline loop\n",
    "\n",
    "for bdt, edt in zip(bdts[:100],edts[:100]):\n",
    "    _df = df[(df['date_time'] >= bdt) & (df['date_time'] < edt) & (df['all_weight'] == 1.0)]\n",
    "    if _df.empty:\n",
    "        continue\n",
    "    else:\n",
    "        processPP(_df,bdt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-03T17:35:02.833993Z",
     "iopub.status.busy": "2021-08-03T17:35:02.833784Z",
     "iopub.status.idle": "2021-08-03T17:35:02.859834Z",
     "shell.execute_reply": "2021-08-03T17:35:02.859128Z",
     "shell.execute_reply.started": "2021-08-03T17:35:02.833971Z"
    }
   },
   "source": [
    "- loop through one tor\n",
    "- add days to a das delayed if they include tornadoes\n",
    "- once delayed arr is of length 12, run a compute...\n",
    "    - Create tor and sigtor prob arrays\n",
    "    - run PAS on these prob arrays and save psv.gz files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Current Implementation of Dask to parallelize via grouping PP creation in chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to find missing dates and process them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-06T19:17:40.068101Z",
     "iopub.status.busy": "2021-08-06T19:17:40.067502Z",
     "iopub.status.idle": "2021-08-06T19:17:40.086813Z",
     "shell.execute_reply": "2021-08-06T19:17:40.079558Z",
     "shell.execute_reply.started": "2021-08-06T19:17:40.068075Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bdt = dt.datetime(2019,1,1,my_hour,my_minute)\n",
    "edt = dt.datetime(2019,12,31,my_hour,my_minute)\n",
    "\n",
    "# Create list index of datetimes with a frequency of one per day\n",
    "dts = pd.date_range(bdt, edt, freq='D')\n",
    "bdts, edts = dts[:-1],dts[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-06T19:17:40.587482Z",
     "iopub.status.busy": "2021-08-06T19:17:40.586809Z",
     "iopub.status.idle": "2021-08-06T19:17:50.278031Z",
     "shell.execute_reply": "2021-08-06T19:17:50.273261Z",
     "shell.execute_reply.started": "2021-08-06T19:17:40.587454Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "da_list = []\n",
    "\n",
    "for bdt, edt in zip(bdts[:],edts[:]):\n",
    "    \n",
    "    file_path = './PAS-climo/output/'\n",
    "    \n",
    "    to_down = f\"{file_path}{bdt.strftime('%Y%m%d%H%M%S')}_ts.psv.gz\"\n",
    "    \n",
    "    _df = df[(df['date_time'] >= bdt) & (df['date_time'] < edt) & (df['all_weight'] == 1.0)]\n",
    "    if _df.empty:\n",
    "        continue\n",
    "    else:\n",
    "        if not bool(glob(to_down)):\n",
    "            \n",
    "            da = dask.delayed(processPP)(_df,bdt)\n",
    "            da_list.append(da)\n",
    "        \n",
    "            if len(da_list) == 2:\n",
    "               dask.compute(da_list)\n",
    "\n",
    "               da_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code to reset the beginning date when dask processing breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-06T15:12:31.112018Z",
     "iopub.status.busy": "2021-08-06T15:12:31.111740Z",
     "iopub.status.idle": "2021-08-06T15:12:31.123547Z",
     "shell.execute_reply": "2021-08-06T15:12:31.122779Z",
     "shell.execute_reply.started": "2021-08-06T15:12:31.111990Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "bdt = dt.datetime(2011,4,27,my_hour,my_minute)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-06T15:12:37.194994Z",
     "iopub.status.busy": "2021-08-06T15:12:37.194773Z",
     "iopub.status.idle": "2021-08-06T15:12:37.203465Z",
     "shell.execute_reply": "2021-08-06T15:12:37.202899Z",
     "shell.execute_reply.started": "2021-08-06T15:12:37.194969Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stuck = bdt\n",
    "edt = dt.datetime(2019,12,31,my_hour,my_minute)\n",
    "\n",
    "# Create list index of datetimes with a frequency of one per day\n",
    "dts = pd.date_range(stuck, edt, freq='D')\n",
    "bdts, edts = dts[:-1],dts[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-06T15:12:38.857239Z",
     "iopub.status.busy": "2021-08-06T15:12:38.857002Z",
     "iopub.status.idle": "2021-08-06T17:03:06.850317Z",
     "shell.execute_reply": "2021-08-06T17:03:06.845595Z",
     "shell.execute_reply.started": "2021-08-06T15:12:38.857215Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 31min 40s, sys: 5min 1s, total: 36min 41s\n",
      "Wall time: 1h 50min 27s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Loop through times and use dask to run in parallel chunks of days at a time\n",
    "\n",
    "da_list = []\n",
    "\n",
    "for bdt, edt in zip(bdts[:],edts[:]):\n",
    "    _df = df[(df['date_time'] >= bdt) & (df['date_time'] < edt) & (df['all_weight'] == 1.0)]\n",
    "    if _df.empty:\n",
    "        continue\n",
    "    else:\n",
    "        da = dask.delayed(processPP)(_df,bdt)\n",
    "        da_list.append(da)\n",
    "        \n",
    "        if len(da_list) == 4:\n",
    "            das = dask.compute(da_list)\n",
    "            \n",
    "            da_list = []\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T17:19:28.702654Z",
     "iopub.status.busy": "2021-08-05T17:19:28.702307Z",
     "iopub.status.idle": "2021-08-05T17:19:28.718477Z",
     "shell.execute_reply": "2021-08-05T17:19:28.717785Z",
     "shell.execute_reply.started": "2021-08-05T17:19:28.702626Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def runPAS(all_fcst, sig_fcst, date_in_name):\n",
    "    \n",
    "    outfile = outdir.joinpath(f\"{date_in_name.strftime('%Y%m%d%H%M%S')}_ts.psv.gz\")\n",
    "    \n",
    "    torn = all_fcst*100\n",
    "    continuous_torn = dc.make_continuous(torn)\n",
    "    sigtorn = sig_fcst*100\n",
    "    \n",
    "    sigtorn[sigtorn > 0] = 1\n",
    "    if (torn.max() >= 30) and (sigtorn.max() > 0):\n",
    "        sigtorn[torn >= 15] += 1\n",
    "    sigtorn_1d = sigtorn.ravel()\n",
    "    usesig = True if (date_in_name.month in coolseason) or (sigtorn.max() > 0) else False\n",
    "    \n",
    "    tornado_dists = dc.TornadoDistributions()\n",
    "    counts = np.zeros((5, nsims), dtype=int)\n",
    "    counts[0, :] = (tornado_dists.f02.rvs(nsims) * ndfd_area * (torn == 2).sum()).astype(int)\n",
    "    counts[1, :] = (tornado_dists.f05.rvs(nsims) * ndfd_area * (torn == 5).sum()).astype(int)\n",
    "    counts[2, :] = (tornado_dists.f10.rvs(nsims) * ndfd_area * (torn == 10).sum()).astype(int)\n",
    "    counts[3, :] = (tornado_dists.f15.rvs(nsims) * ndfd_area * (torn == 15).sum()).astype(int)\n",
    "    counts[4, :] = (tornado_dists.f30.rvs(nsims) * ndfd_area * (torn >= 30).sum()).astype(int)\n",
    "    \n",
    "    ### Setup Impact Simulation ###\n",
    "    igrids = dc.ImpactGrids(impacts_data_root)\n",
    "    \n",
    "    scounts = counts.sum(axis=1)\n",
    "    inds02 = dc.weighted_choice(prob=2, probs=torn, cprobs=continuous_torn, size=scounts[0])\n",
    "    inds05 = dc.weighted_choice(prob=5, probs=torn, cprobs=continuous_torn, size=scounts[1])\n",
    "    inds10 = dc.weighted_choice(prob=10, probs=torn, cprobs=continuous_torn, size=scounts[2])\n",
    "    inds15 = dc.weighted_choice(prob=15, probs=torn, cprobs=continuous_torn, size=scounts[3])\n",
    "    inds30 = dc.weighted_choice(prob=30, probs=torn, cprobs=continuous_torn, size=scounts[4])\n",
    "    inds = dc.flatten_list([inds02, inds05, inds10, inds15, inds30])\n",
    "    \n",
    "    non_sig_inds = sigtorn_1d[inds] == 0\n",
    "    single_sig_inds = sigtorn_1d[inds] == 1\n",
    "    double_sig_inds = sigtorn_1d[inds] == 2\n",
    "    \n",
    "    if usesig:\n",
    "        single_sig_inds += non_sig_inds\n",
    "        non_sig_inds[:] = False\n",
    "        \n",
    "    # Handle Locations\n",
    "    non_sig_loc_inds = inds[non_sig_inds]\n",
    "    single_sig_loc_inds = inds[single_sig_inds]\n",
    "    double_sig_loc_inds = inds[double_sig_inds]\n",
    "    \n",
    "    # Handle Ratings\n",
    "    _mags=[0, 1, 2, 3, 4, 5]\n",
    "    non_sig_ratings = np.random.choice(_mags, size=non_sig_inds.sum(),\n",
    "                                        replace=True, p=tornado_dists.r_nonsig)\n",
    "    single_sig_ratings = np.random.choice(_mags, size=single_sig_inds.sum(),\n",
    "                                            replace=True, p=tornado_dists.r_singlesig)\n",
    "    double_sig_ratings = np.random.choice(_mags, size=double_sig_inds.sum(),\n",
    "                                                replace=True, p=tornado_dists.r_doublesig)\n",
    "    \n",
    "    # Handle Distances\n",
    "    non_sig_distances = dc.get_distances(non_sig_ratings, tornado_dists)\n",
    "    single_sig_distances = dc.get_distances(single_sig_ratings, tornado_dists)\n",
    "    double_sig_distances = dc.get_distances(double_sig_ratings, tornado_dists)\n",
    "    \n",
    "    #print(\"Running simulations...\")\n",
    "    #print(\"    Non Sig...\")\n",
    "    non_sig = dc.simulate(non_sig_loc_inds, non_sig_distances,\n",
    "                            non_sig_ratings, tornado_direction_distribution, igrids)\n",
    "    #print(\"    Single Sig...\")\n",
    "    single_sig = dc.simulate(single_sig_loc_inds, single_sig_distances,\n",
    "                                single_sig_ratings, tornado_direction_distribution, igrids)\n",
    "    #print(\"    Double Sig...\")\n",
    "    double_sig = dc.simulate(double_sig_loc_inds, double_sig_distances,\n",
    "                                double_sig_ratings, tornado_direction_distribution, igrids)\n",
    "    \n",
    "    simulated_tornadoes = dc.flatten_list([non_sig, single_sig, double_sig])\n",
    "    np.random.shuffle(simulated_tornadoes)\n",
    "    _sims = np.split(simulated_tornadoes, counts.sum(axis=0).cumsum())[:-1]\n",
    "    realizations = dc.Realizations([dc.SyntheticTornadoRealization(_sim, i+1) for i, _sim in enumerate(_sims)])\n",
    "    \n",
    "    with gzip.GzipFile(outfile, \"w\") as OUT:\n",
    "        OUT.write(realizations.as_psv.encode())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-08-05T17:19:29.331137Z",
     "iopub.status.busy": "2021-08-05T17:19:29.330862Z",
     "iopub.status.idle": "2021-08-05T17:19:29.340838Z",
     "shell.execute_reply": "2021-08-05T17:19:29.340128Z",
     "shell.execute_reply.started": "2021-08-05T17:19:29.331103Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def processPP(_df,date_in_name):\n",
    "    \n",
    "    lon1 = _df.slon.values\n",
    "    lat1 = _df.slat.values\n",
    "    lon2 = _df.elon.values\n",
    "    lat2 = _df.elat.values\n",
    "\n",
    "    # Find/remove/replace missing data\n",
    "    keep = ~np.logical_or(lon1 == 0, lat1 == 0)\n",
    "    lon1 = lon1[keep]\n",
    "    lat1 = lat1[keep]\n",
    "    lon2 = lon2[keep]\n",
    "    lat2 = lat2[keep]\n",
    "    lon2[lon2 == 0] = lon1[lon2 == 0]\n",
    "    lat2[lat2 == 0] = lat1[lat2 == 0]\n",
    "\n",
    "    # Grid tornadoes\n",
    "    tornlines = G.grid_lines(sxs=lon1, sys=lat1, exs=lon2, eys=lat2)\n",
    "    all_mags = _df['all_weight']\n",
    "    sig_mags = _df['sig_weight']\n",
    "    all_fcst = G.make_empty_grid(dtype='float')\n",
    "    sig_fcst = G.make_empty_grid(dtype='float')\n",
    "    \n",
    "    for tornline, all_mag, sig_mag in zip(tornlines, all_mags, sig_mags):\n",
    "        all_fcst[tornline] = all_mag\n",
    "        sig_fcst[tornline] = sig_mag\n",
    "    \n",
    "    # Make practically perfect forecast\n",
    "    all_fcst = skmorph.binary_dilation(all_fcst, selem).astype(float)\n",
    "    sig_fcst = skmorph.binary_dilation(sig_fcst, selem).astype(float)\n",
    "\n",
    "    #print(np.max(fcst))\n",
    "    all_fcst = ndimage.gaussian_filter(all_fcst, 120/dx)\n",
    "    sig_fcst = ndimage.gaussian_filter(sig_fcst, 120/dx)\n",
    "    \n",
    "    # Degrade continuous probs into SPC prob\n",
    "    # Uncomment if regular probs\n",
    "    all_fcst[all_fcst < 0.02] = 0\n",
    "    all_fcst[np.logical_and(all_fcst < 0.05, all_fcst >= 0.02)] = 0.02\n",
    "    all_fcst[np.logical_and(all_fcst < 0.10, all_fcst >= 0.05)] = 0.05\n",
    "    all_fcst[np.logical_and(all_fcst < 0.15, all_fcst >= 0.10)] = 0.10\n",
    "    all_fcst[np.logical_and(all_fcst < 0.30, all_fcst >= 0.15)] = 0.15\n",
    "    all_fcst[np.logical_and(all_fcst < 0.45, all_fcst >= 0.30)] = 0.30\n",
    "    all_fcst[all_fcst >= 0.45] = 0.45\n",
    "\n",
    "    # Uncomment if sigtor probs\n",
    "    sig_fcst[sig_fcst < 0.10] = 0\n",
    "    sig_fcst[sig_fcst >= 0.10] = 0.10\n",
    "    \n",
    "    # Have this function call PAS function here\n",
    "    runPAS(all_fcst, sig_fcst, date_in_name)\n",
    "    \n",
    "    #return all_fcst, sig_fcst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Trying different ways to chunk data for dask parallelizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "finished = dask.compute(anom(bdts,edts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def anom(bdts,edts):\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for bdt, edt in zip(bdts[:100],edts[:100]):\n",
    "        _df = df[(df['date_time'] >= bdt) & (df['date_time'] < edt) & (df['all_weight'] == 1.0)]\n",
    "        if _df.empty:\n",
    "            continue\n",
    "        else:\n",
    "            result = processPP_da(_df,bdt)\n",
    "            \n",
    "        results.append(result)\n",
    "            \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "@dask.delayed\n",
    "def processPP_da(_df,date_in_name):\n",
    "    \n",
    "    dtime = date_in_name.strftime('%Y%m%d%H%M%S')\n",
    "    \n",
    "    lon1 = _df.slon.values\n",
    "    lat1 = _df.slat.values\n",
    "    lon2 = _df.elon.values\n",
    "    lat2 = _df.elat.values\n",
    "\n",
    "    # Find/remove/replace missing data\n",
    "    keep = ~np.logical_or(lon1 == 0, lat1 == 0)\n",
    "    lon1 = lon1[keep]\n",
    "    lat1 = lat1[keep]\n",
    "    lon2 = lon2[keep]\n",
    "    lat2 = lat2[keep]\n",
    "    lon2[lon2 == 0] = lon1[lon2 == 0]\n",
    "    lat2[lat2 == 0] = lat1[lat2 == 0]\n",
    "\n",
    "    # Grid tornadoes\n",
    "    tornlines = G.grid_lines(sxs=lon1, sys=lat1, exs=lon2, eys=lat2)\n",
    "    all_mags = _df['all_weight']\n",
    "    sig_mags = _df['sig_weight']\n",
    "    all_fcst = G.make_empty_grid(dtype='float')\n",
    "    sig_fcst = G.make_empty_grid(dtype='float')\n",
    "    \n",
    "    for tornline, all_mag, sig_mag in zip(tornlines, all_mags, sig_mags):\n",
    "        all_fcst[tornline] = all_mag\n",
    "        sig_fcst[tornline] = sig_mag\n",
    "    \n",
    "    # Make practically perfect forecast\n",
    "    all_fcst = skmorph.binary_dilation(all_fcst, selem).astype(float)\n",
    "    sig_fcst = skmorph.binary_dilation(sig_fcst, selem).astype(float)\n",
    "\n",
    "    #print(np.max(fcst))\n",
    "    all_fcst = ndimage.gaussian_filter(all_fcst, 120/dx)\n",
    "    sig_fcst = ndimage.gaussian_filter(sig_fcst, 120/dx)\n",
    "    \n",
    "    # Degrade continuous probs into SPC prob\n",
    "    # Uncomment if regular probs\n",
    "    all_fcst[all_fcst < 0.02] = 0\n",
    "    all_fcst[np.logical_and(all_fcst < 0.05, all_fcst >= 0.02)] = 0.02\n",
    "    all_fcst[np.logical_and(all_fcst < 0.10, all_fcst >= 0.05)] = 0.05\n",
    "    all_fcst[np.logical_and(all_fcst < 0.15, all_fcst >= 0.10)] = 0.10\n",
    "    all_fcst[np.logical_and(all_fcst < 0.30, all_fcst >= 0.15)] = 0.15\n",
    "    all_fcst[np.logical_and(all_fcst < 0.45, all_fcst >= 0.30)] = 0.30\n",
    "    all_fcst[all_fcst >= 0.45] = 0.45\n",
    "\n",
    "    # Uncomment if sigtor probs\n",
    "    sig_fcst[sig_fcst < 0.10] = 0\n",
    "    sig_fcst[sig_fcst >= 0.10] = 0.10\n",
    "    \n",
    "    # Have this function call PAS function here\n",
    "    \n",
    "    return [all_fcst, sig_fcst]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
